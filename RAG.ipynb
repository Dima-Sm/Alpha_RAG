{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q-f7MQ13Xv4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -U FlagEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI2OjUnQqc7e"
      },
      "outputs": [],
      "source": [
        "!pip install razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEHh8yQQqAL4"
      },
      "outputs": [],
      "source": [
        "!pip install rank-bm25 pymorphy3 nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U0ZT8OJPEdB"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKD_hrMAwIC9"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLxEqZS8yN1H"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd4Tl504g_VI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
        "from typing import Callable\n",
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import time\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "import pymorphy3\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from razdel import tokenize\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12gz_qpeqO5v"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q3cUd5_zmuS"
      },
      "outputs": [],
      "source": [
        "morph = pymorphy3.MorphAnalyzer()\n",
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "\n",
        "stop_words_rag = [\n",
        "    'который',      # 7220\n",
        "    'клиент',       # 16358\n",
        "    'услуга',       # 7711\n",
        "    'договор',      # 10123\n",
        "    'условие',      # 5132\n",
        "    'случай',       # 3951\n",
        "    'день',         # 5435\n",
        "    'месяц',        # 5352\n",
        "    'год',          # 4836\n",
        "    'срок',         # 4081\n",
        "    'альфа-банк'    # 3966\n",
        "]\n",
        "\n",
        "\n",
        "all_stopwords = russian_stopwords.union(stop_words_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ_-vjf_3yYG"
      },
      "outputs": [],
      "source": [
        "def preprocess_text_for_bm25(text, stopwords, use_lemmatization=True, remove_numbers=True):\n",
        "    \"\"\"\n",
        "    Предобработка текста для BM25 с учетом банковской тематики и маркдаун-разметки\n",
        "\n",
        "    Args:\n",
        "        text: исходный текст (может содержать маркдаун)\n",
        "        use_lemmatization: приводить слова к нормальной форме\n",
        "        remove_numbers: удалять цифры\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Удаление маркдаун-разметки\n",
        "    text = re.sub(r'#+\\s+', ' ', text)  # заголовки\n",
        "    text = re.sub(r'\\*{1,2}(.*?)\\*{1,2}', r'\\1', text)  # жирный/курсив\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)  # ссылки [текст](url)\n",
        "    text = re.sub(r'`{1,3}(.*?)`{1,3}', r'\\1', text)  # код\n",
        "    text = re.sub(r'-\\s*\\[[x\\s]\\]\\s*', ' ', text)  # чекбоксы\n",
        "    text = re.sub(r'\\|.*?\\|', ' ', text)  # таблицы\n",
        "\n",
        "    # 2. Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Удаление смайлов и эмодзи\n",
        "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
        "    text = re.sub(r'[;:]-?[\\)\\(/\\\\|\\[\\]]', '', text)\n",
        "\n",
        "    # 4. Удаление пунктуации (сохраняем дефисы в составных словах)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation.replace('-', '') + '«»—'), ' ', text)\n",
        "\n",
        "    # 5. Удаление цифр\n",
        "    if remove_numbers:\n",
        "        text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "    # 6. Удаление лишних пробелов\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 7. Токенизация и фильтрация\n",
        "    tokens = text.split()\n",
        "    processed_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # Удаляем токены короче 2 символов\n",
        "        if len(token) < 2:\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        # Лемматизация (рекомендуется для русского языка)\n",
        "        if use_lemmatization:\n",
        "            try:\n",
        "                parsed = morph.parse(token)[0]\n",
        "                lemma = parsed.normal_form\n",
        "                # Не добавляем леммы короче 2 символов\n",
        "                if len(lemma) >= 2:\n",
        "                    processed_tokens.append(lemma)\n",
        "            except:\n",
        "                # В случае ошибки оставляем оригинальный токен\n",
        "                processed_tokens.append(token)\n",
        "        else:\n",
        "            processed_tokens.append(token)\n",
        "\n",
        "        # Удаляем стоп-слова\n",
        "        if token in stopwords:\n",
        "            continue\n",
        "\n",
        "    return ' '.join(processed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-ksYtvShGmk"
      },
      "outputs": [],
      "source": [
        "df_questions_clean = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Хакатоны/ALPHA - RAG/questions_clean.csv\")\n",
        "df_sample_submission = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Хакатоны/ALPHA - RAG/sample_submission.csv\")\n",
        "df_websites_updated = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Хакатоны/ALPHA - RAG/websites.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh5BZCvNjOYH"
      },
      "outputs": [],
      "source": [
        "mx = []\n",
        "mx_texts = []\n",
        "c = 0\n",
        "k = 0\n",
        "\n",
        "sharp_texts = []\n",
        "lens_sharp_texts = []\n",
        "for i in range(len(df_websites_updated[\"text\"])):\n",
        "  tmp = str(df_websites_updated.loc[i, \"text\"])\n",
        "  if len(tmp) > 1024:\n",
        "    c += 1\n",
        "    mx_texts.append(tmp)\n",
        "  if \"#\" in tmp:\n",
        "    sharp_texts.append(tmp)\n",
        "    lens_sharp_texts.append(len(tmp))\n",
        "  mx.append(len(tmp))\n",
        "\n",
        "  if len(tmp) > 512:\n",
        "    k += 1\n",
        "\n",
        "print(\"ЗАПИСЕЙ БОЛЬШЕ 1024 СИМВОЛОВ: \", c)\n",
        "print(\"ЗАПИСЕЙ БОЛЬШЕ 512 СИМВОЛОВ: \", k)\n",
        "print(\"ВСЕГО ЗАПИСЕЙ: \", len(df_websites_updated[\"text\"]))\n",
        "\n",
        "mx.sort()\n",
        "print(\"МАКСИМАЛЬНЫЕ ДЛИНЫ ТЕКСТОВ: \")\n",
        "mx[-10:]\n",
        "\n",
        "print(\"ДЛИНЫ ТЕКСТОВ С #: \", lens_sharp_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xProJk2tlY45"
      },
      "outputs": [],
      "source": [
        "mx = []\n",
        "\n",
        "c = 0\n",
        "for i in range(len(df_questions_clean[\"query\"])):\n",
        "  tmp = str(df_questions_clean.iloc[i, 1])\n",
        "\n",
        "  mx.append(len(tmp))\n",
        "\n",
        "mx.sort()\n",
        "print(\"МАКСИМАЛЬНЫЕ ДЛИНЫ ЗАПРОСОВ: \")\n",
        "mx[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q-pGa9CWgkM"
      },
      "outputs": [],
      "source": [
        "web_clone = df_websites_updated.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B40pofRC0B3f"
      },
      "outputs": [],
      "source": [
        "web_clone['text'] = web_clone['text'].apply(\n",
        "    lambda x: preprocess_text_for_bm25(x, stopwords=all_stopwords)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPTL-tDKmdOo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepvk/USER-bge-m3\")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 400,\n",
        "    chunk_overlap = 80,\n",
        "    length_function = lambda x: len(tokenizer.encode(x)),\n",
        "    keep_separator = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTjdCMNwvICD"
      },
      "outputs": [],
      "source": [
        "primary_chunks = []\n",
        "titles = list(df_websites_updated[\"title\"])\n",
        "texts = df_websites_updated[[\"web_id\", \"text\"]]\n",
        "\n",
        "for i in range(len(texts)-1):\n",
        "\n",
        "  chunk = {\"text\": texts.iloc[i, 1], \"id\": int(texts.iloc[i, 0]), \"title\": titles[i]}\n",
        "\n",
        "  primary_chunk= Document(page_content = chunk[\"text\"], metadata = {\"id\": chunk[\"id\"], \"title\": chunk[\"title\"]})\n",
        "\n",
        "  primary_chunks.append(primary_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jupJWlENooYi"
      },
      "outputs": [],
      "source": [
        "rec_chunks = splitter.split_documents(primary_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVw2DpLbxdcf"
      },
      "outputs": [],
      "source": [
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name = \"deepvk/USER-bge-m3\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1yZl1R_yraB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents = rec_chunks,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "\n",
        "vectorstore.save_local(\"/content/drive/MyDrive/Colab Notebooks/Хакатоны/ALPHA - RAG/faiss_index\")\n",
        "\n",
        "print(f\"БД создана, документов: {vectorstore.index.ntotal}\")\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JrImQ6BtLVU"
      },
      "outputs": [],
      "source": [
        "\n",
        "vectorstore = FAISS.load_local(\n",
        "    folder_path=\"/content/drive/MyDrive/Colab Notebooks/Хакатоны/ALPHA - RAG/faiss_index\",\n",
        "    embeddings=embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "print(f\"Документов: {vectorstore.index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEJ6Buntxg6i"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": 150})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFSlspNXpd5S"
      },
      "outputs": [],
      "source": [
        "russian_stopwords = set(stopwords.words(\"russian\"))\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "\n",
        "  tokens = tokenize(text)\n",
        "\n",
        "  return [\n",
        "\n",
        "      token.text for token in tokens\n",
        "      if token.text not in russian_stopwords\n",
        "      and token.text not in string.punctuation\n",
        "      and token.text.isalnum()\n",
        "      and len(token.text) > 1\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1g677VHrzi2"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus = [preprocess_text(doc.page_content) for doc in rec_chunks]\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svBDCxjq6pzR"
      },
      "outputs": [],
      "source": [
        "query = \"Где узнать бик и счёт\"\n",
        "tokenized_query = preprocess_text(query)\n",
        "\n",
        "all_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "scored_docs = []\n",
        "for i, score in enumerate(all_scores):\n",
        "    if score > 0:\n",
        "        scored_docs.append({\n",
        "            'document': rec_chunks[i],\n",
        "            'score': score,\n",
        "            'index': i\n",
        "        })\n",
        "\n",
        "scored_docs.sort(key=lambda x: x['score'], reverse=True)\n",
        "top_docs_with_scores = scored_docs[:5]\n",
        "\n",
        "top_docs_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h2X1WX0Ld9m"
      },
      "outputs": [],
      "source": [
        "bm25_k = 150\n",
        "faiss_k = 150\n",
        "merged_k = 250–300\n",
        "reranker_k = 150–200\n",
        "documents_out = 5\n",
        "\n",
        "score(doc) = max(chunk_scores) + 0.1 * mean(top-3)\n",
        "\n",
        "def rank_documents_from_chunks(query, chunks, reranker, top_chunk_k=100, top_doc_k=5):\n",
        "    # chunks: список dict({\n",
        "    #   'doc_id': str,\n",
        "    #   'chunk': object with .page_content\n",
        "    # })\n",
        "\n",
        "    # Step 1 — ограничить кандидатов\n",
        "    chunks = chunks[:top_chunk_k]\n",
        "\n",
        "    # Step 2 — подготовить пары\n",
        "    pairs = [[query, ch[\"chunk\"].page_content] for ch in chunks]\n",
        "\n",
        "    # Step 3 — получить скор\n",
        "    if reranker.model_type == \"flag\":\n",
        "        scores = reranker.reranker.compute_score(pairs)\n",
        "    else:\n",
        "        scores = reranker.reranker.predict(pairs)\n",
        "\n",
        "    for i, ch in enumerate(chunks):\n",
        "        ch[\"rerank_score\"] = float(scores[i])\n",
        "\n",
        "    # Step 4 — групировка по документам\n",
        "    from collections import defaultdict\n",
        "    doc_scores = defaultdict(list)\n",
        "\n",
        "    for ch in chunks:\n",
        "        doc_scores[ch[\"doc_id\"]].append(ch[\"rerank_score\"])\n",
        "\n",
        "    # Step 5 — агрегированная оценка\n",
        "    final_docs = []\n",
        "    for doc_id, scs in doc_scores.items():\n",
        "        scs_sorted = sorted(scs, reverse=True)\n",
        "        max_score = scs_sorted[0]\n",
        "        mean_top3 = sum(scs_sorted[:3]) / min(len(scs_sorted), 3)\n",
        "        final_score = max_score + 0.1 * mean_top3   # Рекомендованное правило\n",
        "        final_docs.append({\"doc_id\": doc_id, \"score\": final_score})\n",
        "\n",
        "    # Step 6 — сортировка документов\n",
        "    final_docs = sorted(final_docs, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "    return final_docs[:top_doc_k]\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "doc_scores = defaultdict(list)\n",
        "\n",
        "for chunk in reranked_chunks:\n",
        "    doc_id = chunk[\"doc_id\"]\n",
        "    doc_scores[doc_id].append(chunk[\"rerank_score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWD3GofpNj3E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9euow1uyTrC"
      },
      "outputs": [],
      "source": [
        "from FlagEmbedding import FlagReranker\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "\n",
        "class BGEReranker:\n",
        "    def __init__(self, model_name=\"BAAI/bge-reranker-v2-m3\", device=\"cuda\"):\n",
        "        try:\n",
        "            self.reranker = FlagReranker(model_name, use_fp16=True, device=device)\n",
        "            self.model_type = \"flag\"\n",
        "        except:\n",
        "\n",
        "            self.reranker = CrossEncoder(model_name, device=device)\n",
        "            self.model_type = \"crossencoder\"\n",
        "\n",
        "    def rerank(self, query, chunks, top_k=150):\n",
        "\n",
        "      documents = chunks\n",
        "\n",
        "      pairs = [[query, doc[\"chunk\"].page_content] for doc in documents]\n",
        "\n",
        "      if self.model_type == \"flag\":\n",
        "          scores = self.reranker.compute_score(pairs)\n",
        "      else:\n",
        "          scores = self.reranker.predict(pairs)\n",
        "\n",
        "      for i, doc in enumerate(documents):\n",
        "          doc[\"rerank_score\"] = float(scores[i])\n",
        "\n",
        "      from collections import defaultdict\n",
        "      doc_scores = defaultdict(list)\n",
        "\n",
        "      for ch in documents:\n",
        "          doc_scores[ch[\"chunk\"].metadata[\"id\"]].append(ch[\"rerank_score\"])\n",
        "\n",
        "      final_docs = []\n",
        "      for doc_id, scs in doc_scores.items():\n",
        "          scs_sorted = sorted(scs, reverse=True)\n",
        "          max_score = scs_sorted[0]\n",
        "          mean_top3 = sum(scs_sorted[:3]) / min(len(scs_sorted), 3)\n",
        "          final_score = max_score + 0.1 * mean_top3\n",
        "          final_docs.append({\"doc_id\": doc_id, \"score\": final_score})\n",
        "\n",
        "      final_docs = sorted(final_docs, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "\n",
        "      return final_docs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziOU9k499gcb"
      },
      "outputs": [],
      "source": [
        "model = BGEReranker()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mryRIcYc7j7u"
      },
      "outputs": [],
      "source": [
        "def get_final_res(query):\n",
        "\n",
        "  vec_chunks = retriever.invoke(query)\n",
        "\n",
        "  tokenized_query = preprocess_text(query)\n",
        "  bm25_chunks = bm25.get_top_n(tokenized_query, rec_chunks, n = 150)\n",
        "\n",
        "  chunks = []\n",
        "\n",
        "  for i in vec_chunks:\n",
        "    if i not in bm25_chunks:\n",
        "      chunks.append(i)\n",
        "  chunks.extend(bm25_chunks)\n",
        "\n",
        "  print(len(chunks))\n",
        "\n",
        "  n_chunks = [{\"chunk\": chunk} for chunk in chunks]\n",
        "\n",
        "  final_ids = model.rerank(query, n_chunks)\n",
        "\n",
        "  return final_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMD_SbF682_Z"
      },
      "outputs": [],
      "source": [
        "res = get_final_res(\"Где узнать бик и счёт\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9Ezvhp4-6qd"
      },
      "outputs": [],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wDqjctLJAAIA"
      },
      "outputs": [],
      "source": [
        "indexes = []\n",
        "\n",
        "for elem in res:\n",
        "\n",
        "  if elem[\"chunk\"].metadata[\"id\"] not in indexes:\n",
        "    indexes.append(elem[\"chunk\"].metadata[\"id\"])\n",
        "  if len(indexes) >= 5:\n",
        "    break\n",
        "\n",
        "indexes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb2nexUbA23M"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "total_res = []\n",
        "c = 0\n",
        "\n",
        "for i in tqdm(df_questions_clean[\"query\"][3500:5001]):\n",
        "\n",
        "  c += 1\n",
        "\n",
        "  res = get_final_res(i)\n",
        "\n",
        "  total_res.append(res)\n",
        "\n",
        "  if c % 10 == 0:\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/Хакатоны/ALPHA - RAG/res_3.json\", \"w\") as f:\n",
        "      json.dump(total_res, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ0UOKxKYnnb"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Хакатоны/ALPHA - RAG/res.json\", \"w\") as f:\n",
        "      json.dump(total_indexes, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}